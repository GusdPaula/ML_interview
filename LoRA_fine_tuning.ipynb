{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRbQ1ZggUR01xJidkrf2jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GusdPaula/ML_interview/blob/main/LoRA_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queen LoRA Fine-Tunning"
      ],
      "metadata": {
        "id": "_8Loy0TXzTaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the model"
      ],
      "metadata": {
        "id": "nPmteNdWzo1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XQzij3qy0Juv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Tj5nEPf_oD58"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect device\n",
        "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "ehJHsuqV8_mf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to \"talk\" with the model"
      ],
      "metadata": {
        "id": "oqeNL9vvzwat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(prompt):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True\n",
        "  )\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=512\n",
        "  )\n",
        "  generated_ids = [\n",
        "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "  ]\n",
        "\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "c0a9aWprv548"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking if the model know something about the singer"
      ],
      "metadata": {
        "id": "4qJxNvkyz7UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who was Freddie Mercury?\""
      ],
      "metadata": {
        "id": "CoQtFhKTwA5w"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask(prompt)"
      ],
      "metadata": {
        "id": "geLpSbgDwCs9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiWSMceQqV8D",
        "outputId": "f7c8cabe-5bd5-486a-e417-8c9ef76a728c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freddie Mercury (1946â€“2005) was the lead singer of the English rock band Queen and one of the most famous figures in popular music history. He was born Freddie William \"Fred\" Mercury on January 26, 1946, in Manchester, England.\n",
            "\n",
            "Mercury rose to fame as a child with his father's love for pop music. At age 17, he began performing at local clubs and schools. By the time he was 18, he had already become a successful solo artist and an international star.\n",
            "\n",
            "In 1963, Mercury formed Queen with Brian May and John Deacon. The band's debut album, \"A Night at the Opera,\" became a commercial success, and it led to Mercury's rise to superstardom. Throughout their career, Mercury released over 40 studio albums and numerous singles, including hits like \"Bohemian Rhapsody.\"\n",
            "\n",
            "Despite his immense popularity, Mercury struggled with personal issues throughout his life. In 1991, he ended his relationship with May after 18 years of marriage, citing mental health problems. Mercury's death occurred on August 1st, 2005, at the age of 47.\n",
            "\n",
            "Freddie Mercury is remembered not only as a musician but also as a person who brought joy and creativity to many people through his music. His legacy continues to inspire generations of musicians and fans around the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tunning the model"
      ],
      "metadata": {
        "id": "PpcyhS6avzeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model needs to add padding so the shape matches\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "Ygdr1bKN1Rnk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the data\n",
        "train_dataset = load_dataset('json', data_files='train_data.json')\n",
        "test_dataset = load_dataset('json', data_files='test_data.json')\n",
        "\n",
        "import json\n",
        "\n",
        "with open('train_data.json', 'r') as f:\n",
        "    train_data_content = json.load(f)\n",
        "\n",
        "with open('test_data.json', 'r') as f:\n",
        "    test_data_content = json.load(f)\n",
        "\n",
        "print(\"Train data content:\")\n",
        "print(json.dumps(train_data_content, indent=2))\n",
        "\n",
        "print(\"\\nTest data content:\")\n",
        "print(json.dumps(test_data_content, indent=2))"
      ],
      "metadata": {
        "id": "6ZpOYxQi1am6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Tokenize data using chat template\n",
        "def preprocess_function(example):\n",
        "    messages = example[\"messages\"]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    tokenized_output = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    return {\"input_ids\": tokenized_output[\"input_ids\"], \"attention_mask\": tokenized_output[\"attention_mask\"]}\n",
        "\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=False)\n",
        "tokenized_eval_dataset = test_dataset.map(preprocess_function, batched=False)"
      ],
      "metadata": {
        "id": "SiaG0I_m1icS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    warmup_steps=50,\n",
        "    max_steps=200,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    output_dir=\"./qwen2.5-0.5b-finetuned\",\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "# Fine-tune using SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset['train'], # Access the 'train' split of the dataset\n",
        "    eval_dataset=tokenized_eval_dataset['train'],  # Access the 'train' split of the dataset for eval\n",
        "    peft_config=lora_config,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter weights (LoRA)\n",
        "trainer.model.save_pretrained(\"./qwen2.5-0.5b-finetuned\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(\"./qwen2.5-0.5b-finetuned\")"
      ],
      "metadata": {
        "id": "ild2Kodr25Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference function\n",
        "def generate_response_with_model(prompt, model, tokenizer, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\", padding=True, return_attention_mask=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Load base model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Load and merge LoRA adapter\n",
        "lora_path = \"./qwen2.5-0.5b-finetuned\"\n",
        "peft_model = PeftModel.from_pretrained(base_model, lora_path, is_trainable=False)\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n"
      ],
      "metadata": {
        "id": "3M3k7W-T6j2D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =  \"Who was Freddie Mercury?\"\n",
        "\n",
        "print(\"\\nðŸ”¸ Fine-tuned LoRA model output:\")\n",
        "print(generate_response_with_model(prompt=prompt, model=merged_model, tokenizer=tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoIAL6kQ9Fz2",
        "outputId": "3baec026-c14a-4372-e31c-d76681d29bf1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¸ Fine-tuned LoRA model output:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Who was Freddie Mercury?\n",
            "assistant\n",
            "Freddie Mercury was the lead singer of the British rock band Queen, known for his powerful voice, flamboyant stage presence, and songwriting. He was born in 1946 and passed away in 1991.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWv-dlur9Mia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}